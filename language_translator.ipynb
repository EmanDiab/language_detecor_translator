{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6P7JAfhhDiAT",
        "outputId": "d814526e-ab74-4909-d3b2-b543c392caaa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "72OgZPT1EE4a"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EqYyyJglEIQK"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"/drive/My Drive/Job Tasks/kemet/translation_train.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qqkkJMc_1Z4",
        "outputId": "c070f3a9-15f3-4578-c912-db7d19084aa7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "293/293 [==============================] - 7050s 24s/step - loss: 1.1857 - val_loss: 0.8490\n",
            "Epoch 2/5\n",
            "293/293 [==============================] - 7079s 24s/step - loss: 0.7961 - val_loss: 0.6078\n",
            "Epoch 3/5\n",
            "293/293 [==============================] - 7026s 24s/step - loss: 0.6121 - val_loss: 0.5925\n",
            "Epoch 4/5\n",
            "293/293 [==============================] - 7026s 24s/step - loss: 0.6075 - val_loss: 0.5910\n",
            "Epoch 5/5\n",
            "293/293 [==============================] - 7095s 24s/step - loss: 0.6060 - val_loss: 0.5901\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Prepare the data\n",
        "arabic_sentences = data[\"Arabic\"].to_list()  # List of Arabic sentences\n",
        "english_sentences = data[\"English\"].to_list()  # List of English sentences\n",
        "\n",
        "# Tokenize the sentences\n",
        "arabic_tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "arabic_tokenizer.fit_on_texts(arabic_sentences)\n",
        "arabic_sequences = arabic_tokenizer.texts_to_sequences(arabic_sentences)\n",
        "\n",
        "english_tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "english_tokenizer.fit_on_texts(english_sentences)\n",
        "english_sequences = english_tokenizer.texts_to_sequences(english_sentences)\n",
        "\n",
        "# Add '<start>' and '<end>' tokens to the English tokenizer vocabulary\n",
        "english_tokenizer.word_index['<start>'] = len(english_tokenizer.word_index) + 1\n",
        "english_tokenizer.word_index['<end>'] = len(english_tokenizer.word_index) + 1\n",
        "english_tokenizer.index_word[english_tokenizer.word_index['<start>']] = '<start>'\n",
        "english_tokenizer.index_word[english_tokenizer.word_index['<end>']] = '<end>'\n",
        "\n",
        "# Pad the sequences\n",
        "arabic_sequences = tf.keras.preprocessing.sequence.pad_sequences(arabic_sequences)\n",
        "english_sequences = tf.keras.preprocessing.sequence.pad_sequences(english_sequences)\n",
        "\n",
        "# Define the maximum sequence length\n",
        "MAX_SEQUENCE_LENGTH = max(len(sequence) for sequence in arabic_sequences)\n",
        "\n",
        "# Prepare the input and output data\n",
        "encoder_input_data = arabic_sequences\n",
        "decoder_input_data = english_sequences[:, :-1]\n",
        "decoder_output_data = english_sequences[:, 1:]\n",
        "\n",
        "# Define the model architecture\n",
        "encoder_inputs = Input(shape=(None,))\n",
        "encoder_embedding = tf.keras.layers.Embedding(len(arabic_tokenizer.word_index) + 1, 256)(encoder_inputs)\n",
        "encoder_lstm = LSTM(256, return_state=True)\n",
        "encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
        "encoder_states = [state_h, state_c]\n",
        "\n",
        "decoder_inputs = Input(shape=(None,))\n",
        "decoder_embedding = tf.keras.layers.Embedding(len(english_tokenizer.word_index) + 1, 256)(decoder_inputs)\n",
        "decoder_lstm = LSTM(256, return_sequences=True, return_state=True)\n",
        "decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "decoder_dense = Dense(len(english_tokenizer.word_index) + 1, activation='softmax')\n",
        "decoder_outputs = decoder_dense(decoder_outputs)\n",
        "\n",
        "# Create the model\n",
        "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
        "\n",
        "# Compile and train the model\n",
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n",
        "model.fit([encoder_input_data, decoder_input_data], decoder_output_data, batch_size=64, epochs=5, validation_split=0.2)\n",
        "\n",
        "# Save the model\n",
        "model.save('translation_model.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "\n",
        "# Load the trained model\n",
        "model = tf.keras.models.load_model('translation_model.h5')\n",
        "\n",
        "# Define the maximum sequence length\n",
        "MAX_SEQUENCE_LENGTH = 256\n",
        "\n",
        "# Translate a sentence\n",
        "def translate(sentence):\n",
        "    # Preprocess the input sentence\n",
        "    sequence = arabic_tokenizer.texts_to_sequences([sentence])\n",
        "    sequence = tf.keras.preprocessing.sequence.pad_sequences(sequence, maxlen=256)\n",
        "\n",
        "    # Generate the translation\n",
        "    encoder_model = tf.keras.models.Model(inputs=model.input[0], outputs=model.layers[4].output)\n",
        "    decoder_lstm = model.layers[5]\n",
        "    decoder_dense = model.layers[6]\n",
        "\n",
        "    encoder_output, state_h_enc, state_c_enc = encoder_model.predict(sequence)\n",
        "    states_value = [state_h_enc, state_c_enc]\n",
        "\n",
        "    target_seq = np.zeros((1, 1,256))\n",
        "    target_seq[0, 0,0] = english_tokenizer.word_index['<start>']  # Start token\n",
        "    stop_condition = False\n",
        "    translation = ''\n",
        "\n",
        "    while not stop_condition:\n",
        "        decoder_output, state_h, state_c = decoder_lstm(target_seq, initial_state=[tf.convert_to_tensor(state_h_enc), tf.convert_to_tensor(state_c_enc)])\n",
        "        decoder_output = decoder_dense(decoder_output)\n",
        "        sampled_token_index = np.argmax(decoder_output[0, -1, :])\n",
        "        if sampled_token_index not in english_tokenizer.index_word:\n",
        "            break\n",
        "        print(sampled_token_index)    \n",
        "        sampled_word = english_tokenizer.index_word[sampled_token_index]\n",
        "        print(sampled_word)\n",
        "        translation += sampled_word + ' '  # Add the sampled word to the translation\n",
        "        print(translation)\n",
        "        if sampled_word == '<end>' or len(translation.split()) >= 256:\n",
        "            stop_condition = True\n",
        "        else:\n",
        "            target_seq = np.zeros((1, 1))\n",
        "            target_seq[0, 0] = sampled_token_index  # Update the target sequence\n",
        "            states_value = [state_h, state_c]\n",
        "\n",
        "    return translation.strip()\n",
        "\n",
        "# Example usage:\n",
        "arabic_text = \"مرحبًا، كيف حالك؟\"\n",
        "english_translation = translate(arabic_text)\n",
        "print(\"Arabic:\", arabic_text)\n",
        "print(\"English:\", english_translation)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98XQ-JgHF8JP",
        "outputId": "1fa8dbde-967f-448a-d998-4890ca840ee5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 526ms/step\n",
            "Arabic: مرحبًا، كيف حالك؟\n",
            "English: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# My comment\n",
        "\n",
        "There is something wrong with how i handle encoder decoder stuff i am still not able to figure it out, yet it got me interested so i will probably continue to work on it till i figure it out. \n",
        "Also the training takes enormous amount of time despite using colab pro so I didn’t have much chances to try multiple stuff\n",
        "of course we could always do the fine tuning thing i thought about it too but didn't have much time to try it\n",
        "\n",
        "Thanks for this enjoyable task hope to hear from you soon!\n"
      ],
      "metadata": {
        "id": "G-6p7cy9el6Y"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y9YiGLf8Ded_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}